{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Set the device as cuda if cuda is available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64  # Size of the input sequence \n",
    "batch_size = 32  # Amount of inputs the model would train on in one single iteration\n",
    "max_iters = 3000\n",
    "lr = 3e-4  # Learning rate (defines the rate with which the model would converge)\n",
    "eval_iters = 10\n",
    "# eval_interval = 500\n",
    "n_embd = 384  # Number of embedding tokens generated per input character\n",
    "n_layer = 4   # Number of decoder layers\n",
    "n_head = 4    # Number of attention layers running in parallel\n",
    "dropout = 0.2  # dropout to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "with open('openwebtext/character_vocab.txt', 'r', encoding= 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "\n",
    "vocab_size = len(chars)  # Getting all characters present in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda i : ''.join([int_to_string[l] for l in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 70, 77, 77, 80]\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"openwebtext/train.txt\" if split=='train' else \"openwebtext/val.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)  # Fetches a random chunk of data of size (batch_size*block_size) from the whole dataset\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size, ))  # Randomly generates starting positions of the input sequences for a whole batch\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # For every starting position, it generates input sequence\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])   # For every starting position, it generates output sequence by offsetting one character from the input sequence\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head of attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # Lower triangular mask is used because for each timestep, we don't want the model to learn from the future timesteps\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        sim_score = torch.matmul(q,k.transpose(-2,-1))*k.shape[-1]**0.5\n",
    "        sim_score = sim_score.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        sim_score = F.softmax(sim_score, dim=-1)\n",
    "        sim_score = self.dropout(sim_score)\n",
    "        attention_score = torch.matmul(sim_score,v)\n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(head_size*n_head, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x+y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire decoder-only transformer model with multi-head attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        \n",
    "        emb = self.token_embedding_table(index)\n",
    "        B, T, C = emb.shape\n",
    "        pos = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = emb + pos\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        B, T, C = logits.shape\n",
    "        \n",
    "        if targets==None:\n",
    "            loss = None\n",
    "        else:\n",
    "            \n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "        \n",
    "\n",
    "    def generate(self,index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            \n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index,index_next), dim=1)\n",
    "            # print(index.shape)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0: train_loss : 0.0\n",
      "iter: 10: train_loss : 7.7714479619806465\n",
      "iter: 20: train_loss : 7.076375688825335\n",
      "iter: 30: train_loss : 6.210507539010817\n",
      "iter: 40: train_loss : 5.504483473010179\n",
      "iter: 50: train_loss : 5.010800908593571\n",
      "iter: 60: train_loss : 4.653175076500315\n",
      "iter: 70: train_loss : 4.395377468055402\n",
      "iter: 80: train_loss : 4.202511145744794\n",
      "iter: 90: train_loss : 4.040774091259464\n",
      "iter: 100: train_loss : 3.916393775751095\n",
      "iter: 110: train_loss : 3.8227935610590755\n",
      "iter: 120: train_loss : 3.7344073382290928\n",
      "iter: 130: train_loss : 3.6543749452547263\n",
      "iter: 140: train_loss : 3.584963800213861\n",
      "iter: 150: train_loss : 3.5298744605866488\n",
      "iter: 160: train_loss : 3.477072164879082\n",
      "iter: 170: train_loss : 3.4284398750952114\n",
      "iter: 180: train_loss : 3.3864025345164768\n",
      "iter: 190: train_loss : 3.3472317188822163\n",
      "iter: 200: train_loss : 3.3095176718128263\n",
      "iter: 210: train_loss : 3.276161826617345\n",
      "iter: 220: train_loss : 3.2457380747902986\n",
      "iter: 230: train_loss : 3.219999326771988\n",
      "iter: 240: train_loss : 3.193477499039836\n",
      "iter: 250: train_loss : 3.172764467528142\n",
      "iter: 260: train_loss : 3.1495495454561664\n",
      "iter: 270: train_loss : 3.1279352203946273\n",
      "iter: 280: train_loss : 3.1096213076038293\n",
      "iter: 290: train_loss : 3.093184578459697\n",
      "iter: 300: train_loss : 3.0753283302649312\n",
      "iter: 310: train_loss : 3.062072953993868\n",
      "iter: 320: train_loss : 3.0485331039191035\n",
      "iter: 330: train_loss : 3.0339759563031152\n",
      "iter: 340: train_loss : 3.020029502879839\n",
      "iter: 350: train_loss : 3.008962936890431\n",
      "iter: 360: train_loss : 2.9977019275654717\n",
      "iter: 370: train_loss : 2.9850346630795626\n",
      "iter: 380: train_loss : 2.973240763496539\n",
      "iter: 390: train_loss : 2.964212224306658\n",
      "iter: 400: train_loss : 2.9525293804463604\n",
      "iter: 410: train_loss : 2.942924951404841\n",
      "iter: 420: train_loss : 2.9336419394350393\n",
      "iter: 430: train_loss : 2.9244796267241724\n",
      "iter: 440: train_loss : 2.9156662725807587\n",
      "iter: 450: train_loss : 2.9100243209999577\n",
      "iter: 460: train_loss : 2.9035538049685465\n",
      "iter: 470: train_loss : 2.8959720676365426\n",
      "iter: 480: train_loss : 2.8879903369039113\n",
      "iter: 490: train_loss : 2.880882200544089\n",
      "iter: 500: train_loss : 2.873846257756094\n",
      "iter: 510: train_loss : 2.868353557213412\n",
      "iter: 520: train_loss : 2.8623717805931985\n",
      "iter: 530: train_loss : 2.856139503629868\n",
      "iter: 540: train_loss : 2.850582125887633\n",
      "iter: 550: train_loss : 2.8441274585827725\n",
      "iter: 560: train_loss : 2.840550863381588\n",
      "iter: 570: train_loss : 2.8384483858499343\n",
      "iter: 580: train_loss : 2.8329340406211028\n",
      "iter: 590: train_loss : 2.82833377278194\n",
      "iter: 600: train_loss : 2.824386262258952\n",
      "iter: 610: train_loss : 2.8194614034237526\n",
      "iter: 620: train_loss : 2.813048673712689\n",
      "iter: 630: train_loss : 2.8072411549261367\n",
      "iter: 640: train_loss : 2.8014477575067054\n",
      "iter: 650: train_loss : 2.7962566004370766\n",
      "iter: 660: train_loss : 2.7931752125542393\n",
      "iter: 670: train_loss : 2.7878313093000657\n",
      "iter: 680: train_loss : 2.7828938782477692\n",
      "iter: 690: train_loss : 2.7785161969975696\n",
      "iter: 700: train_loss : 2.7799621805144783\n",
      "iter: 710: train_loss : 2.7762165830775825\n",
      "iter: 720: train_loss : 2.7726460911860578\n",
      "iter: 730: train_loss : 2.768096461667897\n",
      "iter: 740: train_loss : 2.7631938389920996\n",
      "iter: 750: train_loss : 2.758507766672838\n",
      "iter: 760: train_loss : 2.753557160712103\n",
      "iter: 770: train_loss : 2.748609734880321\n",
      "iter: 780: train_loss : 2.7439685047970235\n",
      "iter: 790: train_loss : 2.740336312958323\n",
      "iter: 800: train_loss : 2.736123064781694\n",
      "iter: 810: train_loss : 2.7318765573231123\n",
      "iter: 820: train_loss : 2.7269799410207844\n",
      "iter: 830: train_loss : 2.722612519556865\n",
      "iter: 840: train_loss : 2.718801746186972\n",
      "iter: 850: train_loss : 2.71479357675716\n",
      "iter: 860: train_loss : 2.7100505823319243\n",
      "iter: 870: train_loss : 2.7058911602751667\n",
      "iter: 880: train_loss : 2.7011829407612935\n",
      "iter: 890: train_loss : 2.6964025344912854\n",
      "iter: 900: train_loss : 2.692099665431151\n",
      "iter: 910: train_loss : 2.6889114796003053\n",
      "iter: 920: train_loss : 2.684952323263295\n",
      "iter: 930: train_loss : 2.6933130165431987\n",
      "iter: 940: train_loss : 2.6897389085589256\n",
      "iter: 950: train_loss : 2.687102841027277\n",
      "iter: 960: train_loss : 2.6839555354321782\n",
      "iter: 970: train_loss : 2.6808081878815324\n",
      "iter: 980: train_loss : 2.6773889558152444\n",
      "iter: 990: train_loss : 2.6742193645954613\n",
      "iter: 1000: train_loss : 2.6705479252707587\n",
      "iter: 1010: train_loss : 2.666826129077568\n",
      "iter: 1020: train_loss : 2.66431325049405\n",
      "iter: 1030: train_loss : 2.6613091740298342\n",
      "iter: 1040: train_loss : 2.657380493642274\n",
      "iter: 1050: train_loss : 2.6538067608079268\n",
      "iter: 1060: train_loss : 2.6512758435223263\n",
      "iter: 1070: train_loss : 2.64801079797255\n",
      "iter: 1080: train_loss : 2.644554690229573\n",
      "iter: 1090: train_loss : 2.6409458801573726\n",
      "iter: 1100: train_loss : 2.6371898170388035\n",
      "iter: 1110: train_loss : 2.633396875740278\n",
      "iter: 1120: train_loss : 2.631878231382072\n",
      "iter: 1130: train_loss : 2.6290847563722513\n",
      "iter: 1140: train_loss : 2.6268758989028194\n",
      "iter: 1150: train_loss : 2.6242398229710027\n",
      "iter: 1160: train_loss : 2.621445875143204\n",
      "iter: 1170: train_loss : 2.6181810093577553\n",
      "iter: 1180: train_loss : 2.615389387452128\n",
      "iter: 1190: train_loss : 2.612245722841956\n",
      "iter: 1200: train_loss : 2.609276189692908\n",
      "iter: 1210: train_loss : 2.6063767631225208\n",
      "iter: 1220: train_loss : 2.604428306746737\n",
      "iter: 1230: train_loss : 2.6024177355266604\n",
      "iter: 1240: train_loss : 2.599440417877615\n",
      "iter: 1250: train_loss : 2.5965831824820294\n",
      "iter: 1260: train_loss : 2.593809628656797\n",
      "iter: 1270: train_loss : 2.59201198820989\n",
      "iter: 1280: train_loss : 2.589435844287381\n",
      "iter: 1290: train_loss : 2.5873678101761963\n",
      "iter: 1300: train_loss : 2.5856590813439593\n",
      "iter: 1310: train_loss : 2.583560168152088\n",
      "iter: 1320: train_loss : 2.581031421203671\n",
      "iter: 1330: train_loss : 2.57893914015646\n",
      "iter: 1340: train_loss : 2.577106573082101\n",
      "iter: 1350: train_loss : 2.5760448460751864\n",
      "iter: 1360: train_loss : 2.5743369145221697\n",
      "iter: 1370: train_loss : 2.5728923418850553\n",
      "iter: 1380: train_loss : 2.5705146435634716\n",
      "iter: 1390: train_loss : 2.568668413470753\n",
      "iter: 1400: train_loss : 2.5661971487376114\n",
      "iter: 1410: train_loss : 2.564005480515543\n",
      "iter: 1420: train_loss : 2.5624811224833413\n",
      "iter: 1430: train_loss : 2.5607617068007476\n",
      "iter: 1440: train_loss : 2.5590197176340963\n",
      "iter: 1450: train_loss : 2.5571413608685103\n",
      "iter: 1460: train_loss : 2.559616767568673\n",
      "iter: 1470: train_loss : 2.558620477841064\n",
      "iter: 1480: train_loss : 2.5569524491340383\n",
      "iter: 1490: train_loss : 2.555746368079438\n",
      "iter: 1500: train_loss : 2.554565693838131\n",
      "iter: 1510: train_loss : 2.553351343427566\n",
      "iter: 1520: train_loss : 2.552280215822028\n",
      "iter: 1530: train_loss : 2.5510198906463084\n",
      "iter: 1540: train_loss : 2.549566621829905\n",
      "iter: 1550: train_loss : 2.5481779456984066\n",
      "iter: 1560: train_loss : 2.5470738213922184\n",
      "iter: 1570: train_loss : 2.545421282703295\n",
      "iter: 1580: train_loss : 2.5478577598750327\n",
      "iter: 1590: train_loss : 2.54796627225702\n",
      "iter: 1600: train_loss : 2.5473778795853472\n",
      "iter: 1610: train_loss : 2.5467428751127974\n",
      "iter: 1620: train_loss : 2.54555266363837\n",
      "iter: 1630: train_loss : 2.54431752147008\n",
      "iter: 1640: train_loss : 2.5430965637158214\n",
      "iter: 1650: train_loss : 2.542486504018126\n",
      "iter: 1660: train_loss : 2.5414834151707235\n",
      "iter: 1670: train_loss : 2.5400777463781696\n",
      "iter: 1680: train_loss : 2.53878231193252\n",
      "iter: 1690: train_loss : 2.537532253705278\n",
      "iter: 1700: train_loss : 2.5357947559794\n",
      "iter: 1710: train_loss : 2.534792484977941\n",
      "iter: 1720: train_loss : 2.5336994853288473\n",
      "iter: 1730: train_loss : 2.532478227290303\n",
      "iter: 1740: train_loss : 2.5313625241202367\n",
      "iter: 1750: train_loss : 2.5298794367053317\n",
      "iter: 1760: train_loss : 2.528753597854146\n",
      "iter: 1770: train_loss : 2.5275758181097547\n",
      "iter: 1780: train_loss : 2.5272900362084383\n",
      "iter: 1790: train_loss : 2.525973106881615\n",
      "iter: 1800: train_loss : 2.52487576332177\n",
      "iter: 1810: train_loss : 2.5239196229152165\n",
      "iter: 1820: train_loss : 2.523882673704513\n",
      "iter: 1830: train_loss : 2.5241255818788373\n",
      "iter: 1840: train_loss : 2.523588968449996\n",
      "iter: 1850: train_loss : 2.5230309375874227\n",
      "iter: 1860: train_loss : 2.5224806967760403\n",
      "iter: 1870: train_loss : 2.522099677793869\n",
      "iter: 1880: train_loss : 2.5214666858877925\n",
      "iter: 1890: train_loss : 2.5208100736740087\n",
      "iter: 1900: train_loss : 2.5201483767387556\n",
      "iter: 1910: train_loss : 2.5194933496801597\n",
      "iter: 1920: train_loss : 2.5191381065749923\n",
      "iter: 1930: train_loss : 2.5184161435872516\n",
      "iter: 1940: train_loss : 2.5173695264312177\n",
      "iter: 1950: train_loss : 2.5161968156168366\n",
      "iter: 1960: train_loss : 2.5152295092913888\n",
      "iter: 1970: train_loss : 2.5142854138233526\n",
      "iter: 1980: train_loss : 2.5135232351573653\n",
      "iter: 1990: train_loss : 2.51303430815428\n",
      "iter: 2000: train_loss : 2.512048711066601\n",
      "iter: 2010: train_loss : 2.511271935324951\n",
      "iter: 2020: train_loss : 2.5100947696934943\n",
      "iter: 2030: train_loss : 2.509665583695525\n",
      "iter: 2040: train_loss : 2.509594842886469\n",
      "iter: 2050: train_loss : 2.5089652412754915\n",
      "iter: 2060: train_loss : 2.5087675583241116\n",
      "iter: 2070: train_loss : 2.5081405199880704\n",
      "iter: 2080: train_loss : 2.5076735087279687\n",
      "iter: 2090: train_loss : 2.5074607010152223\n",
      "iter: 2100: train_loss : 2.506927517355765\n",
      "iter: 2110: train_loss : 2.506776562916165\n",
      "iter: 2120: train_loss : 2.5064722513714135\n",
      "iter: 2130: train_loss : 2.505955700357438\n",
      "iter: 2140: train_loss : 2.5056282105372367\n",
      "iter: 2150: train_loss : 2.5050985365344003\n",
      "iter: 2160: train_loss : 2.5041736442145788\n",
      "iter: 2170: train_loss : 2.5034267002950887\n",
      "iter: 2180: train_loss : 2.5029423377864792\n",
      "iter: 2190: train_loss : 2.5020927440516214\n",
      "iter: 2200: train_loss : 2.5012119376404836\n",
      "iter: 2210: train_loss : 2.5004698523888487\n",
      "iter: 2220: train_loss : 2.500097261039806\n",
      "iter: 2230: train_loss : 2.499522181595991\n",
      "iter: 2240: train_loss : 2.4988981347592585\n",
      "iter: 2250: train_loss : 2.4985807903710073\n",
      "iter: 2260: train_loss : 2.4988955337882093\n",
      "iter: 2270: train_loss : 2.4984957240636736\n",
      "iter: 2280: train_loss : 2.498164063950789\n",
      "iter: 2290: train_loss : 2.498370829860699\n",
      "iter: 2300: train_loss : 2.498071830617713\n",
      "iter: 2310: train_loss : 2.497348581439538\n",
      "iter: 2320: train_loss : 2.496679139003729\n",
      "iter: 2330: train_loss : 2.4959607610166556\n",
      "iter: 2340: train_loss : 2.4951476273705744\n",
      "iter: 2350: train_loss : 2.4943040189008823\n",
      "iter: 2360: train_loss : 2.493463786463271\n",
      "iter: 2370: train_loss : 2.4940169769618543\n",
      "iter: 2380: train_loss : 2.494467349975663\n",
      "iter: 2390: train_loss : 2.494084098557457\n",
      "iter: 2400: train_loss : 2.493712719357644\n",
      "iter: 2410: train_loss : 2.494130827192942\n",
      "iter: 2420: train_loss : 2.493991756478601\n",
      "iter: 2430: train_loss : 2.4936447327934337\n",
      "iter: 2440: train_loss : 2.493629966491268\n",
      "iter: 2450: train_loss : 2.4936082335891943\n",
      "iter: 2460: train_loss : 2.4934948099090626\n",
      "iter: 2470: train_loss : 2.4934494594773122\n",
      "iter: 2480: train_loss : 2.4930492776480957\n",
      "iter: 2490: train_loss : 2.4931916317658365\n",
      "iter: 2500: train_loss : 2.4931695394542683\n",
      "iter: 2510: train_loss : 2.493043768125813\n",
      "iter: 2520: train_loss : 2.493244671812137\n",
      "iter: 2530: train_loss : 2.493024628042751\n",
      "iter: 2540: train_loss : 2.4927660708631976\n",
      "iter: 2550: train_loss : 2.4924729004975443\n",
      "iter: 2560: train_loss : 2.492081357184354\n",
      "iter: 2570: train_loss : 2.491806779747944\n",
      "iter: 2580: train_loss : 2.4917890374303004\n",
      "iter: 2590: train_loss : 2.4915895872436313\n",
      "iter: 2600: train_loss : 2.4914551606227784\n",
      "iter: 2610: train_loss : 2.4912834410373392\n",
      "iter: 2620: train_loss : 2.490867020291529\n",
      "iter: 2630: train_loss : 2.491165048674638\n",
      "iter: 2640: train_loss : 2.4911998286927934\n",
      "iter: 2650: train_loss : 2.4917039962949863\n",
      "iter: 2660: train_loss : 2.4919543081725823\n",
      "iter: 2670: train_loss : 2.4920211436372712\n",
      "iter: 2680: train_loss : 2.4923606480026104\n",
      "iter: 2690: train_loss : 2.492619802942067\n",
      "iter: 2700: train_loss : 2.493370410381799\n",
      "iter: 2710: train_loss : 2.4937285614470985\n",
      "iter: 2720: train_loss : 2.4939829532121394\n",
      "iter: 2730: train_loss : 2.494146198547179\n",
      "iter: 2740: train_loss : 2.494169822360251\n",
      "iter: 2750: train_loss : 2.494355389180074\n",
      "iter: 2760: train_loss : 2.4944323660972456\n",
      "iter: 2770: train_loss : 2.495535521763664\n",
      "iter: 2780: train_loss : 2.4972842086008753\n",
      "iter: 2790: train_loss : 2.4974252078235044\n",
      "iter: 2800: train_loss : 2.497314242199888\n",
      "iter: 2810: train_loss : 2.49752856861523\n",
      "iter: 2820: train_loss : 2.4976459202805295\n",
      "iter: 2830: train_loss : 2.498243909048891\n",
      "iter: 2840: train_loss : 2.4983080573922853\n",
      "iter: 2850: train_loss : 2.49858679942021\n",
      "iter: 2860: train_loss : 2.498748309709775\n",
      "iter: 2870: train_loss : 2.498834441240471\n",
      "iter: 2880: train_loss : 2.498978978103749\n",
      "iter: 2890: train_loss : 2.4989440267946184\n",
      "iter: 2900: train_loss : 2.4997562080858002\n",
      "iter: 2910: train_loss : 2.499773507427817\n",
      "iter: 2920: train_loss : 2.500037197052963\n",
      "iter: 2930: train_loss : 2.5004877460080177\n",
      "iter: 2940: train_loss : 2.504609715812914\n",
      "iter: 2950: train_loss : 2.5053373048039704\n",
      "iter: 2960: train_loss : 2.506110954349084\n",
      "iter: 2970: train_loss : 2.506830297949658\n",
      "iter: 2980: train_loss : 2.50749299183103\n",
      "iter: 2990: train_loss : 2.507975937809206\n",
      "2.6627676486968994\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "cummulative_loss = 0\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter%eval_iters==0:\n",
    "        # losses = estimate_loss()\n",
    "        # print(f\"iter: {iter}: train_loss : {losses['train']}, eval_loss : {losses['val']}\")\n",
    "        print(f\"iter: {iter}: train_loss : {cummulative_loss/(iter+1)}\")\n",
    "    if iter%100==0:\n",
    "        checkpoint = {\n",
    "            'epoch': iter,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': cummulative_loss/(iter+1),  # Example: saving the last recorded loss\n",
    "        }\n",
    "        torch.save(checkpoint, 'model_checkpoint.pth')\n",
    "    \n",
    "    x,y = get_batch('train')\n",
    "    logits, loss = model.forward(x,y)\n",
    "    cummulative_loss = cummulative_loss + loss.item()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing and loading the model from model_checkpoint.pth file. (Can be used to load a previously trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4997562080858002\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['train_loss']\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Or7)0%s, o::aul- 7584170966 wal LSAA= gTLnzá‹¢ MEldyre mar let\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "# print(context.shape)\n",
    "string = model.generate(context, max_new_tokens = 60)[0].tolist()\n",
    "generated_chars = decode(string)\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
